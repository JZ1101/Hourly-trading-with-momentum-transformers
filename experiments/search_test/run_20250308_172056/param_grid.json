{
    "seq_length": [
        8,
        13,
        21,
        34,
        55,
        89,
        144,
        233
    ],
    "hidden_dim": [
        64,
        128,
        256,
        512,
        768
    ],
    "num_heads": [
        4,
        8,
        16,
        24
    ],
    "num_layers": [
        2,
        4,
        6,
        8
    ],
    "dropout": [
        0.1,
        0.2,
        0.3,
        0.4,
        0.5
    ],
    "learning_rate": [
        1e-05,
        5e-05,
        0.0001,
        0.0005,
        0.001
    ],
    "batch_size": [
        16,
        32,
        64,
        128
    ],
    "patience": [
        10,
        15,
        20,
        25
    ]
}