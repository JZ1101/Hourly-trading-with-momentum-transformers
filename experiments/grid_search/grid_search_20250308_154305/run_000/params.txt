{
    "seq_length": 144,
    "patience": 10,
    "num_layers": 8,
    "num_heads": 4,
    "learning_rate": 0.0005,
    "hidden_dim": 256,
    "dropout": 0.5,
    "batch_size": 32
}